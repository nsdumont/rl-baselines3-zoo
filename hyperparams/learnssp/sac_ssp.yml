Ant-v4:
  learning_starts: 10000
  n_timesteps: 1000000.0
  policy: MlpPolicy
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))

ContinuousMaze-5x5-v0: &conmaze-defaults
  batch_size: 512
  buffer_size: 10000
  ent_coef: 0.1
  gamma: 0.9999
  gradient_steps: 32
  learning_rate: 0.0003
  learning_starts: 0
  n_timesteps: 100000.0
  policy: MlpPolicy
  policy_kwargs: dict(log_std_init=-3.67, net_arch=[64, 64],features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=151,ssp_h=[1.0,
    1.0]))
  tau: 0.01
  train_freq: 32
  use_sde: true
  
ContinuousMaze-6x6-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-7x7-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-8x8-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-9x9-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-10x10-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-11x11-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-12x12-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-15x15-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 5e5
  
ContinuousMaze-20x20-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 1e6
  
  
  
ContinuousMazeBlocks-5x5-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-6x6-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-7x7-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-8x8-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-9x9-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-10x10-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-11x11-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-12x12-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-15x15-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 5e5
  
ContinuousMazeBlocks-19x19-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 1e6 
  
  

HalfCheetah-v4: &mujoco-defaults #17 dim
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_starts: 10000
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))

# Ant-v4:
#   <<: *mujoco-defaults

#tuned
Hopper-v4: #11
  <<: *mujoco-defaults
  gamma: 0.99
  learning_rate: 0.0004983758833984422
  batch_size: 2048
  buffer_size: 1000000
  learning_starts: 1000
  train_freq: 512
  tau: 0.02
  policy_kwargs: dict(log_std_init=-0.18224045124845256,net_arch=[400, 300],features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))

Walker2d-v4: #17
  <<: *mujoco-defaults
  gamma: 0.98
  learning_rate: 0.004235227770813154
  batch_size: 512
  buffer_size: 1000000
  learning_starts: 20000
  train_freq: 128
  tau: 0.005
  policy_kwargs: dict(log_std_init=0.09831834642485887,net_arch=[400, 300],features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=577,basis_type="hex"))


# Humanoid-v4:
#   <<: *mujoco-defaults
#   n_timesteps: !!float 2e6

Swimmer-v4: # 8
  <<: *mujoco-defaults
  gamma: 0.9999
  
  
LunarLanderContinuous-v2:
  batch_size: 256
  buffer_size: 1000000
  ent_coef: auto
  gamma: 0.99
  gradient_steps: 1
  learning_rate: lin_7.3e-4
  learning_starts: 10000
  n_timesteps: 500000.0
  policy: MlpPolicy
  policy_kwargs: dict(net_arch=[400, 300],features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=451))
  tau: 0.01
  train_freq: 1
MountainCarContinuous-v0:
  batch_size: 512
  buffer_size: 50000
  ent_coef: 0.1
  gamma: 0.9999
  gradient_steps: 32
  learning_rate: 0.0003
  learning_starts: 0
  n_timesteps: 50000.0
  policy: MlpPolicy
  policy_kwargs: dict(log_std_init=-3.67, net_arch=[64, 64],features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=151,ssp_h=[0.34728204118474704,
    0.15569961421392498]))
  tau: 0.01
  train_freq: 32
  use_sde: true
Pendulum-v1:
  learning_rate: 0.001
  n_timesteps: 20000
  policy: MlpPolicy
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=201))

