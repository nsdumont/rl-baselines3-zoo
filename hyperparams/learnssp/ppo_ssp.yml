Acrobot-v1:
  batch_size: 8
  clip_range: 0.3
  ent_coef: 0.0016887662574405905
  gae_lambda: 0.98
  gamma: 0.98
  learning_rate: 0.0014122296715315709
  max_grad_norm: 0.8
  n_envs: 16
  n_epochs: 1
  n_steps: 32
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(net_arch=dict(pi=[64, 64], vf=[64, 64]),activation_fn=nn.ReLU,ortho_init=False,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=225,ssp_h=[0.028142760643669197,
    16.769420234485487, 1.1949794271332383, 0.19860115953574767, 0.11197384800365881,
    0.3108735565198609]))
  vf_coef: 0.19001510460449456
Ant-v4:
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))
CartPole-v1:
  batch_size: 256
  clip_range: lin_0.2
  ent_coef: 0.0
  gae_lambda: 0.8
  gamma: 0.98
  learning_rate: lin_0.001
  n_envs: 8
  n_epochs: 20
  n_steps: 32
  n_timesteps: 100000.0
  policy: MlpPolicy
  policy_kwargs: dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=251,ssp_h=[0.96000004,
    0.1, 0.083775806, 0.1]))

ContinuousMaze-5x5-v0: &conmaze-defaults
  batch_size: 128
  clip_range: 0.4
  ent_coef: 2.3498743376649477e-07
  gae_lambda: 0.98
  gamma: 0.99
  learning_rate: 0.001
  max_grad_norm: 1
  n_envs: 16
  n_epochs: 10
  n_steps: 512
  n_timesteps: 200000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(activation_fn=nn.ReLU, net_arch=dict(pi=[64], vf=[64]), features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=151,ssp_h=[1.0,1.0]))
  vf_coef: 0.5726555590232599

ContinuousMaze-6x6-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-7x7-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-8x8-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-9x9-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-10x10-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-11x11-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-12x12-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-15x15-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-20x20-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 1e6
  
  
  
ContinuousMazeBlocks-5x5-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-6x6-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-7x7-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-8x8-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-9x9-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-10x10-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-11x11-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-12x12-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-15x15-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 5e5
  
ContinuousMazeBlocks-19x19-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 1e6
  
  
ContinuousMazeBlocks-20x20-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5

HalfCheetah-v4:
  batch_size: 64
  clip_range: 0.1
  ent_coef: 0.000401762
  gae_lambda: 0.92
  gamma: 0.98
  learning_rate: 2.0633e-05
  max_grad_norm: 0.8
  n_envs: 1
  n_epochs: 20
  n_steps: 512
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict( log_std_init=-2, ortho_init=False, activation_fn=nn.ReLU, net_arch=dict(pi=[256,
    256], vf=[256, 256]) ,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))
  vf_coef: 0.58096
Hopper-v4:
  batch_size: 32
  clip_range: 0.2
  ent_coef: 0.00229519
  gae_lambda: 0.99
  gamma: 0.999
  learning_rate: 9.80828e-05
  max_grad_norm: 0.7
  n_envs: 1
  n_epochs: 5
  n_steps: 512
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict( log_std_init=-2, ortho_init=False, activation_fn=nn.ReLU, net_arch=dict(pi=[256,
    256], vf=[256, 256]) ,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))
  vf_coef: 0.835671
LunarLander-v2:
  batch_size: 64
  ent_coef: 0.01
  gae_lambda: 0.98
  gamma: 0.999
  n_envs: 16
  n_epochs: 4
  n_steps: 1024
  n_timesteps: 1000000.0
  policy: MlpPolicy
  policy_kwargs: dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=451))
LunarLanderContinuous-v2:
  batch_size: 64
  ent_coef: 0.01
  gae_lambda: 0.98
  gamma: 0.999
  n_envs: 16
  n_epochs: 4
  n_steps: 1024
  n_timesteps: 1000000.0
  policy: MlpPolicy
  policy_kwargs: dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=451))
MountainCar-v0:
  ent_coef: 0.0
  gae_lambda: 0.98
  gamma: 0.99
  n_envs: 16
  n_epochs: 4
  n_steps: 16
  n_timesteps: 1000000.0
  policy: MlpPolicy
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=151,ssp_h=[0.34728204118474704,
    0.15569961421392498]))
MountainCarContinuous-v0:
  batch_size: 256
  clip_range: 0.1
  ent_coef: 0.00429
  gae_lambda: 0.9
  gamma: 0.9999
  learning_rate: 7.77e-05
  max_grad_norm: 5
  n_envs: 1
  n_epochs: 10
  n_steps: 8
  n_timesteps: 20000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(log_std_init=-3.29, ortho_init=False,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=151))
  use_sde: true
  vf_coef: 0.19
Pendulum-v1:
  clip_range: 0.2
  ent_coef: 0.0
  gae_lambda: 0.95
  gamma: 0.9
  learning_rate: 0.001
  n_envs: 4
  n_epochs: 10
  n_steps: 1024
  n_timesteps: 100000.0
  policy: MlpPolicy
  policy_kwargs: dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU,features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=73,ssp_h=[0.23239569118125408,
    1.451069567569478, 1.1845291313041482]))
  sde_sample_freq: 4
  use_sde: true
Swimmer-v4:
  batch_size: 256
  gae_lambda: 0.98
  gamma: 0.9999
  learning_rate: 0.0006
  n_envs: 4
  n_steps: 1024
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))
Walker2d-v4:
  batch_size: 32
  clip_range: 0.1
  ent_coef: 0.000585045
  gae_lambda: 0.95
  gamma: 0.99
  learning_rate: 5.05041e-05
  max_grad_norm: 1
  n_envs: 1
  n_epochs: 20
  n_steps: 512
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))
  vf_coef: 0.871923
  
InvertedDoublePendulum-v4:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 512
  n_steps: 128
  gamma: 0.98
  learning_rate: 0.000155454
  ent_coef: 1.05057e-06
  clip_range: 0.4
  n_epochs: 10
  gae_lambda: 0.8
  max_grad_norm: 0.5
  vf_coef: 0.695929
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))

InvertedPendulum-v4:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 64
  n_steps: 32
  gamma: 0.999
  learning_rate: 0.000222425
  ent_coef: 1.37976e-07
  clip_range: 0.4
  n_epochs: 5
  gae_lambda: 0.9
  max_grad_norm: 0.3
  vf_coef: 0.19816
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))

Reacher-v4:
  normalize: true
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e6
  batch_size: 32
  n_steps: 512
  gamma: 0.9
  learning_rate: 0.000104019
  ent_coef: 7.52585e-08
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 1.0
  max_grad_norm: 0.9
  vf_coef: 0.950368
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPProcesser,features_extractor_kwargs=dict(features_dim=501,basis_type="rand"))
  
MiniGrid-Empty-Random-5x5-v0: &minigrid-defaults
  env_wrapper: 
  - hrr_gym_wrappers.PrepMiniGridViewFlatWrapper:
      shape_out: 201
  n_envs: 8 # number of environment copies running in parallel
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  n_steps: 128 # batch size is n_steps * n_env
  batch_size: 64 # Number of training minibatches per update
  gae_lambda: 0.95 #  Factor for trade-off of bias vs variance for Generalized Advantage Estimator
  gamma: 0.99
  n_epochs: 10 #  Number of epoch when optimizing the surrogate
  ent_coef: 0.0 # Entropy coefficient for the loss caculation
  learning_rate: 2.5e-4 # The learning rate, it can be a function
  clip_range: 0.2 # Clipping parameter, it can be a function
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPMiniGridViewProcesser,features_extractor_kwargs=dict(features_dim=201, ssp_h=[1.,1.,0.1]))


MiniGrid-Empty-5x5-v0:
  <<: *minigrid-defaults
  
MiniGrid-Empty-8x8-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5
  
MiniGrid-Empty-16x16-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5

MiniGrid-FourRooms-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e6


MiniGrid-DoorKey-5x5-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5
  
MiniGrid-DoorKey-6x6-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5
  
MiniGrid-DoorKey-8x8-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e6

MiniGrid-MultiRoom-N4-S5-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e7 # Unsolved
  
MiniGrid-Unlock-v0:
  <<: *minigrid-defaults
  
MiniGrid-SimpleCrossingS11N5-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5

MiniGrid-Fetch-5x5-N2-v0: &babyai-defaults
  n_envs: 8 # number of environment copies running in parallel
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  n_steps: 128 # batch size is n_steps * n_env
  batch_size: 64 # Number of training minibatches per update
  gae_lambda: 0.95 #  Factor for trade-off of bias vs variance for Generalized Advantage Estimator
  gamma: 0.99
  n_epochs: 10 #  Number of epoch when optimizing the surrogate
  ent_coef: 0.0 # Entropy coefficient for the loss caculation
  learning_rate: 2.5e-4 # The learning rate, it can be a function
  clip_range: 0.2 # Clipping parameter, it can be a function
  env_wrapper: 
  - hrr_gym_wrappers.PrepBabyAIFlatWrapper:
      shape_out: 201
  n_timesteps: !!float 5e6
  policy_kwargs: dict(features_extractor_class=hrr_gym_wrappers.SSPBabyAIViewProcesser,features_extractor_kwargs=dict(features_dim=201, ssp_h=[1.,1.,0.1]))


MiniGrid-GoToDoor-5x5-v0:
  <<: *babyai-defaults
  n_timesteps: !!float 5e6

MiniGrid-PutNear-6x6-N2-v0:
  <<: *babyai-defaults
  n_timesteps: !!float 1e7

MiniGrid-RedBlueDoors-6x6-v0:
  <<: *babyai-defaults
  n_timesteps: !!float 1e6
  n_steps: 512

MiniGrid-LockedRoom-v0:
  <<: *babyai-defaults
  env_wrapper: 
  n_timesteps: !!float 1e7 # Unsolved

MiniGrid-KeyCorridorS3R1-v0:
  <<: *babyai-defaults
  n_timesteps: !!float 5e5

BabyAI-GoToLocalS8N7-v0:
  <<: *babyai-defaults
  n_timesteps: !!float 5e5

MiniGrid-ObstructedMaze-2Dlh-v0:
  <<: *babyai-defaults
  n_timesteps: !!float 1e7 # Unsolved
