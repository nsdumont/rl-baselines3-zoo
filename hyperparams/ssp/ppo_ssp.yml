#tuned
Acrobot-v1:
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 225
      length_scale: [0.028142760643669197, 16.769420234485487,  1.1949794271332383,  0.19860115953574767,  0.11197384800365881,  0.3108735565198609]
  batch_size: 8
  gae_lambda: 0.98
  learning_rate: 0.0014122296715315709
  gamma: 0.98
  n_envs: 16
  ent_coef: 0.0016887662574405905
  clip_range: 0.3
  max_grad_norm: 0.8
  vf_coef: 0.19001510460449456
  n_epochs: 1
  n_steps: 32
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: "dict(net_arch=dict(pi=[64, 64], vf=[64, 64]),activation_fn=nn.ReLU,ortho_init=False)"

CartPole-v1:
  batch_size: 256
  clip_range: lin_0.2
  ent_coef: 0.0
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 251
  gae_lambda: 0.8
  gamma: 0.98
  learning_rate: lin_0.001
  n_envs: 8
  n_epochs: 20
  n_steps: 32
  n_timesteps: 100000.0
  policy: MlpPolicy
  
LunarLander-v2:
  batch_size: 64
  ent_coef: 0.01
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 451
  gae_lambda: 0.98
  gamma: 0.999
  n_envs: 16
  n_epochs: 4
  n_steps: 1024
  n_timesteps: 1000000.0
  policy: MlpPolicy
  
LunarLanderContinuous-v2:
  batch_size: 64
  ent_coef: 0.01
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 451
  gae_lambda: 0.98
  gamma: 0.999
  n_envs: 16
  n_epochs: 4
  n_steps: 1024
  n_timesteps: 1000000.0
  policy: MlpPolicy
  
MountainCar-v0:
  ent_coef: 0.0
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
  gae_lambda: 0.98
  gamma: 0.99
  n_envs: 16
  n_epochs: 4
  n_steps: 16
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  
MountainCarContinuous-v0:
  batch_size: 256
  clip_range: 0.1
  ent_coef: 0.00429
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
  gae_lambda: 0.9
  gamma: 0.9999
  learning_rate: 7.77e-05
  max_grad_norm: 5
  n_envs: 1
  n_epochs: 10
  n_steps: 8
  n_timesteps: 20000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(log_std_init=-3.29, ortho_init=False)
  use_sde: true
  vf_coef: 0.19
  
Pendulum-v1:
  clip_range: 0.2
  ent_coef: 0.0
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 201
  gae_lambda: 0.95
  gamma: 0.9
  learning_rate: 0.001
  n_envs: 4
  n_epochs: 10
  n_steps: 1024
  n_timesteps: 100000.0
  policy: MlpPolicy
  sde_sample_freq: 4
  use_sde: true
  

MiniGrid-Empty-Random-5x5-v0: &minigrid-defaults-xy
  env_wrapper: 
  - hrr_gym_wrappers.SSPMiniGridXYFlatWrapper:
      shape_out: 201
  n_envs: 8 # number of environment copies running in parallel
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  n_steps: 128 # batch size is n_steps * n_env
  batch_size: 64 # Number of training minibatches per update
  gae_lambda: 0.95 #  Factor for trade-off of bias vs variance for Generalized Advantage Estimator
  gamma: 0.99
  n_epochs: 10 #  Number of epoch when optimizing the surrogate
  ent_coef: 0.0 # Entropy coefficient for the loss caculation
  learning_rate: 2.5e-4 # The learning rate, it can be a function
  clip_range: 0.2 # Clipping parameter, it can be a function
  
MiniGrid-Empty-5x5-v0:
  <<: *minigrid-defaults-xy
  
MiniGrid-FourRooms-v0: &minigrid-defaults-view
  env_wrapper:
  - hrr_gym_wrappers.SSPMiniGridViewFlatWrapper:
      shape_out: 201
  n_envs: 8 # number of environment copies running in parallel
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  n_steps: 512 # batch size is n_steps * n_env
  batch_size: 64 # Number of training minibatches per update
  gae_lambda: 0.95 #  Factor for trade-off of bias vs variance for Generalized Advantage Estimator
  gamma: 0.99
  n_epochs: 10 #  Number of epoch when optimizing the surrogate
  ent_coef: 0.0 # Entropy coefficient for the loss caculation
  learning_rate: 2.5e-4 # The learning rate, it can be a function
  clip_range: 0.2 # Clipping parameter, it can be a function

MiniGrid-DoorKey-5x5-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 1e5

MiniGrid-MultiRoom-N4-S5-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 1e7 # Unsolved

MiniGrid-Fetch-5x5-N2-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 5e6

MiniGrid-GoToDoor-5x5-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 5e6

MiniGrid-PutNear-6x6-N2-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 1e7

MiniGrid-RedBlueDoors-6x6-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 1e6
  n_steps: 512

MiniGrid-LockedRoom-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 1e7 # Unsolved

MiniGrid-KeyCorridorS3R1-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 5e5

MiniGrid-Unlock-v0:
  <<: *minigrid-defaults-view

MiniGrid-ObstructedMaze-2Dlh-v0:
  <<: *minigrid-defaults-view
  n_timesteps: !!float 1e7 # Unsolved
