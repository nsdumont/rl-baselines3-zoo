#tuned, kinda
Acrobot-v1:
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 225
      length_scale: [0.028142760643669197, 16.769420234485487,  1.1949794271332383,  0.19860115953574767,  0.11197384800365881,  0.3108735565198609]
  batch_size: 8
  gae_lambda: 0.98
  learning_rate: 0.0014122296715315709
  gamma: 0.98
  n_envs: 16
  ent_coef: 0.0016887662574405905
  clip_range: 0.3
  max_grad_norm: 0.8
  vf_coef: 0.19001510460449456
  n_epochs: 1
  n_steps: 32
  n_timesteps: 1000000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: "dict(net_arch=dict(pi=[64, 64], vf=[64, 64]),activation_fn=nn.ReLU,ortho_init=False)"

CartPole-v1:
  batch_size: 256
  clip_range: lin_0.2
  ent_coef: 0.0
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 251
      length_scale: [9.6000004e-01, 1.0000000e-01, 8.3775806e-02, 1.0000000e-01]
  gae_lambda: 0.8
  gamma: 0.98
  learning_rate: lin_0.001
  n_envs: 8
  n_epochs: 20
  n_steps: 32
  n_timesteps: 100000.0
  policy: MlpPolicy
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU)" # new
  
LunarLander-v2:
  batch_size: 64
  ent_coef: 0.01
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 451
  gae_lambda: 0.98
  gamma: 0.999
  n_envs: 16
  n_epochs: 4
  n_steps: 1024
  n_timesteps: 1000000.0
  policy: MlpPolicy
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU)"
  
LunarLanderContinuous-v2:
  batch_size: 64
  ent_coef: 0.01
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 451
  gae_lambda: 0.98
  gamma: 0.999
  n_envs: 16
  n_epochs: 4
  n_steps: 1024
  n_timesteps: 1000000.0
  policy: MlpPolicy
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU)"
  
MountainCar-v0:
  ent_coef: 0.0
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
      length_scale: [0.34728204118474704, 0.15569961421392498]
  gae_lambda: 0.98
  gamma: 0.99
  n_envs: 16
  n_epochs: 4
  n_steps: 16
  n_timesteps: 1000000.0
  policy: MlpPolicy
  
MountainCarContinuous-v0:
  batch_size: 256
  clip_range: 0.1
  ent_coef: 0.00429
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
  gae_lambda: 0.9
  gamma: 0.9999
  learning_rate: 7.77e-05
  max_grad_norm: 5
  n_envs: 1
  n_epochs: 10
  n_steps: 8
  n_timesteps: 20000.0
  normalize: true
  policy: MlpPolicy
  policy_kwargs: dict(log_std_init=-3.29, ortho_init=False)
  use_sde: true
  vf_coef: 0.19
  
Pendulum-v1:
  clip_range: 0.2
  ent_coef: 0.0
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 73
      length_scale: [0.23239569118125408, 1.451069567569478, 1.1845291313041482]
  gae_lambda: 0.95
  gamma: 0.9
  learning_rate: 0.001
  n_envs: 4
  n_epochs: 10
  n_steps: 1024
  n_timesteps: 100000.0
  policy: MlpPolicy
  sde_sample_freq: 4
  use_sde: true
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU)"
  


# Following https://github.com/lcswillems/rl-starter-files
MiniGrid-Empty-Random-5x5-v0: &minigrid-defaults
  env_wrapper: 
  - hrr_gym_wrappers.SSPMiniGridViewFlatWrapper:
      shape_out: 201
      length_scale: [1.0000000, 1.0000000, 0.1000000]
  n_envs: 8 # number of environment copies running in parallel
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  n_steps: 128 # batch size is n_steps * n_env
  batch_size: 64 # Number of training minibatches per update
  gae_lambda: 0.95 #  Factor for trade-off of bias vs variance for Generalized Advantage Estimator
  gamma: 0.99
  n_epochs: 10 #  Number of epoch when optimizing the surrogate
  ent_coef: 0.0 # Entropy coefficient for the loss caculation
  learning_rate: 2.5e-4 # The learning rate, it can be a function
  clip_range: 0.2 # Clipping parameter, it can be a function

MiniGrid-Empty-5x5-v0:
  <<: *minigrid-defaults
  
MiniGrid-Empty-8x8-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5
  
MiniGrid-Empty-16x16-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5

MiniGrid-FourRooms-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e6


MiniGrid-DoorKey-5x5-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5
  
MiniGrid-DoorKey-6x6-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e5
  
MiniGrid-DoorKey-8x8-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e6

MiniGrid-MultiRoom-N4-S5-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e7 # Unsolved

MiniGrid-Fetch-5x5-N2-v0:
  <<: *minigrid-defaults
  env_wrapper: 
  - hrr_gym_wrappers.SSPBabyAIFlatWrapper:
      shape_out: 201
      length_scale: [1.0000000, 1.0000000, 0.1000000]
  n_timesteps: !!float 5e6

MiniGrid-GoToDoor-5x5-v0:
  <<: *minigrid-defaults
  env_wrapper: 
  - hrr_gym_wrappers.SSPBabyAIFlatWrapper:
      shape_out: 201
      length_scale: [1.0000000, 1.0000000, 0.1000000]
  n_timesteps: !!float 5e6

MiniGrid-PutNear-6x6-N2-v0:
  <<: *minigrid-defaults
  env_wrapper: 
  - hrr_gym_wrappers.SSPBabyAIFlatWrapper:
      shape_out: 201
      length_scale: [1.0000000, 1.0000000, 0.1000000]
  n_timesteps: !!float 1e7

MiniGrid-RedBlueDoors-6x6-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e6
  n_steps: 512

MiniGrid-LockedRoom-v0:
  <<: *minigrid-defaults
  env_wrapper: 
  - hrr_gym_wrappers.SSPBabyAIFlatWrapper: # need to change wrapper to handle mulit-step instructions
      shape_out: 201
      length_scale: [1.0000000, 1.0000000, 0.1000000]
  n_timesteps: !!float 1e7 # Unsolved

MiniGrid-KeyCorridorS3R1-v0:
  <<: *minigrid-defaults
  env_wrapper: 
  - hrr_gym_wrappers.SSPBabyAIFlatWrapper:
      shape_out: 201
      length_scale: [1.0000000, 1.0000000, 0.1000000]
  n_timesteps: !!float 5e5

MiniGrid-Unlock-v0:
  <<: *minigrid-defaults

MiniGrid-ObstructedMaze-2Dlh-v0:
  <<: *minigrid-defaults
  n_timesteps: !!float 1e7 # Unsolved

# MiniGrid-Empty-Random-5x5-v0: &minigrid-defaults-xy
#   env_wrapper: 
#   - hrr_gym_wrappers.SSPMiniGridXYFlatWrapper:
#       shape_out: 201
#       length_scale: [0.5000000, 0.5000000, 0.5000000]
#   n_envs: 8 # number of environment copies running in parallel
#   n_timesteps: !!float 1e5
#   policy: 'MlpPolicy'
#   n_steps: 128 # batch size is n_steps * n_env
#   batch_size: 64 # Number of training minibatches per update
#   gae_lambda: 0.95 #  Factor for trade-off of bias vs variance for Generalized Advantage Estimator
#   gamma: 0.99
#   n_epochs: 10 #  Number of epoch when optimizing the surrogate
#   ent_coef: 0.0 # Entropy coefficient for the loss caculation
#   learning_rate: 2.5e-4 # The learning rate, it can be a function
#   clip_range: 0.2 # Clipping parameter, it can be a function
  
# MiniGrid-Empty-5x5-v0:
#   <<: *minigrid-defaults-xy
  
# MiniGrid-FourRooms-v0: &minigrid-defaults-view
#   env_wrapper:
#   - hrr_gym_wrappers.SSPMiniGridViewFlatWrapper:
#       shape_out: 201
#       length_scale: [0.5000000, 0.5000000, 0.5000000]
#   n_envs: 8 # number of environment copies running in parallel
#   n_timesteps: !!float 5e6
#   policy: 'MlpPolicy'
#   n_steps: 512 # batch size is n_steps * n_env
#   batch_size: 64 # Number of training minibatches per update
#   gae_lambda: 0.95 #  Factor for trade-off of bias vs variance for Generalized Advantage Estimator
#   gamma: 0.99
#   n_epochs: 10 #  Number of epoch when optimizing the surrogate
#   ent_coef: 0.0 # Entropy coefficient for the loss caculation
#   learning_rate: 2.5e-4 # The learning rate, it can be a function
#   clip_range: 0.2 # Clipping parameter, it can be a function


# MiniGrid-DoorKey-5x5-v0:
#   env_wrapper: 
#   - hrr_gym_wrappers.SSPMiniGridViewFlatWrapper:
#       shape_out: 129
#   n_envs: 2
#   n_steps: 25
#   learning_rate: 0.001
#   ent_coef: 0.001
#   n_timesteps: !!float 5e4
#   max_grad_norm: 10
#   batch_size: 256
#   policy: MlpPolicy
#   policy_kwargs: "dict(features_extractor_class=MlpFeaturesExtractor, features_extractor_kwargs=dict(features_dim=64,net_arch=[64],activation_fn=nn.ReLU), net_arch=dict(pi=[64], vf=[64]),activation_fn=nn.ReLU)"

# MiniGrid-MultiRoom-N4-S5-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 1e7 # Unsolved

# MiniGrid-Fetch-5x5-N2-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 5e6

# MiniGrid-GoToDoor-5x5-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 5e6

# MiniGrid-PutNear-6x6-N2-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 1e7

# MiniGrid-RedBlueDoors-6x6-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 1e6
#   n_steps: 512

# MiniGrid-LockedRoom-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 1e7 # Unsolved

# MiniGrid-KeyCorridorS3R1-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 5e5

# MiniGrid-Unlock-v0:
#   <<: *minigrid-defaults-view

# MiniGrid-ObstructedMaze-2Dlh-v0:
#   <<: *minigrid-defaults-view
#   n_timesteps: !!float 1e7 # Unsolved


maze-sample-5x5-v0: &maze-defaults
    env_wrapper:
    - hrr_gym_wrappers.SSPObsWrapper:
        shape_out: 151
        length_scale: [1.0000000, 1.0000000] 
    n_timesteps: !!float 1.5e4
    policy: 'MlpPolicy'
    n_steps: 1024 
    batch_size: 128 
    gae_lambda: 1.0
    gamma: 0.95
    n_epochs: 10 
    ent_coef: 2.6555164799018923e-07
    learning_rate: 2.6555164799018923e-07
    clip_range: 0.2 
    max_grad_norm: 1
    vf_coef: 0.15524873265910033

maze-random-5x5-v0:
  <<: *maze-defaults
  
maze-random-6x6-v0:
  <<: *maze-defaults
  
maze-sample-7x7-v0:
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
      length_scale: [1.0000000, 1.0000000] 
  n_envs: 1 # number of environment copies running in parallel
  n_timesteps: !!float 1.5e4
  policy: 'MlpPolicy'
  n_steps: 128 
  batch_size: 64 
  gae_lambda: 0.9
  gamma: 0.99
  n_epochs: 5 
  ent_coef: 6.0e-5
  learning_rate: 5.0e-3
  clip_range: 0.2 
  max_grad_norm: 0.7
  vf_coef: 0.5827230291956254
  
maze-random-7x7-v0:
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
      length_scale: [1.0000000, 1.0000000] 
  n_envs: 1 # number of environment copies running in parallel
  n_timesteps: !!float 1.5e4
  policy: 'MlpPolicy'
  n_steps: 128 
  batch_size: 64 
  gae_lambda: 0.9
  gamma: 0.99
  n_epochs: 5 
  ent_coef: 6.0e-5
  learning_rate: 5.0e-3
  clip_range: 0.2 
  max_grad_norm: 0.7
  vf_coef: 0.5827230291956254
  
maze-sample-8x8-v0:
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
      length_scale: [1.0000000, 1.0000000] 
  n_envs: 1 # number of environment copies running in parallel
  n_timesteps: !!float 2.0e4
  policy: 'MlpPolicy'
  n_steps: 1024 
  batch_size: 64 
  gae_lambda: 0.8
  gamma: 0.99
  n_epochs: 10 
  ent_coef: 2.6364810667123056e-4
  learning_rate: 5.0e-4
  clip_range: 0.2 
  max_grad_norm: 0.6
  vf_coef: 0.36730903178120766
  
maze-random-8x8-v0:
  <<: *maze-defaults
  n_timesteps: !!float 2e4

maze-sample-9x9-v0:
  <<: *maze-defaults
  n_timesteps: !!float 2.5e4
    
maze-random-9x9-v0:
  <<: *maze-defaults
  n_timesteps: !!float 2.5e4
  
maze-random-10x10-v0:
  <<: *maze-defaults
  n_timesteps: !!float 2.5e4
  
maze-sample-10x10-v0:
  <<: *maze-defaults
  n_timesteps: !!float 2e4


ContinuousMaze-5x5-v0: &conmaze-defaults
  env_wrapper:
  - hrr_gym_wrappers.SSPObsWrapper:
      shape_out: 151
      length_scale: [1.0000000, 1.0000000] 
  normalize: true
  n_envs: 16
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  batch_size: 128
  n_steps: 512
  gamma: 0.99
  learning_rate: 0.001
  ent_coef: 2.3498743376649477e-07
  clip_range: 0.4
  n_epochs: 10
  gae_lambda: 0.98
  max_grad_norm: 1
  vf_coef: 0.5726555590232599
  policy_kwargs: "dict(activation_fn=nn.ReLU,
                       net_arch=dict(pi=[64], vf=[64]),
                       )"
                       
ContinuousMaze-6x6-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-7x7-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-8x8-v0:
  <<: *conmaze-defaults
  
ContinuousMaze-9x9-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-10x10-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-11x11-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-12x12-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMaze-15x15-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 5e5
  
ContinuousMaze-20x20-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 1e6
  
  
  
ContinuousMazeBlocks-5x5-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-6x6-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-7x7-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-8x8-v0:
  <<: *conmaze-defaults
  
ContinuousMazeBlocks-9x9-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-10x10-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-11x11-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-12x12-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 3e5
  
ContinuousMazeBlocks-15x15-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 5e5
  
ContinuousMazeBlocks-19x19-v0:
  <<: *conmaze-defaults
  n_timesteps: !!float 1e6